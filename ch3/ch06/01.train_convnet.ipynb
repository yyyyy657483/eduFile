{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 합성곱 신경망   \n",
        "CNN 클래스로 학습 및 검증을 구현하자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1720232461098
        }
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "print(os.getcwd())\n",
        "current_dir = os.path.dirname(os.getcwd())\n",
        "print(current_dir)\n",
        "os.chdir(current_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.295937919574106\n",
            "=== epoch:1, train acc:0.131, test acc:0.155 ===\n",
            "train loss:2.2935989260448717\n",
            "train loss:2.285769482699653\n",
            "train loss:2.2800153313230815\n",
            "train loss:2.2652421008424324\n",
            "train loss:2.24401418123409\n",
            "train loss:2.2485549236763895\n",
            "train loss:2.2017224863944125\n",
            "train loss:2.1933577125921486\n",
            "train loss:2.159963156915787\n",
            "train loss:2.1215034453423764\n",
            "train loss:2.0728903563885424\n",
            "train loss:1.9856077997365305\n",
            "train loss:1.90928374878767\n",
            "train loss:1.8746745369481288\n",
            "train loss:1.813955381824637\n",
            "train loss:1.792993117745304\n",
            "train loss:1.726076171464916\n",
            "train loss:1.6658094399742303\n",
            "train loss:1.4415529964937464\n",
            "train loss:1.5156353595596446\n",
            "train loss:1.2439182441132708\n",
            "train loss:1.2636393995381396\n",
            "train loss:1.2695859424157108\n",
            "train loss:1.1687825193258115\n",
            "train loss:1.1192225748799738\n",
            "train loss:1.034858238662295\n",
            "train loss:0.8957530880630493\n",
            "train loss:1.0473767674029437\n",
            "train loss:0.9013170161869963\n",
            "train loss:0.8155781432012015\n",
            "train loss:0.8080431014344192\n",
            "train loss:0.7641432745227305\n",
            "train loss:0.7816381540555153\n",
            "train loss:0.738501250124031\n",
            "train loss:0.7913046073544043\n",
            "train loss:0.7467986349108401\n",
            "train loss:0.6726027598265841\n",
            "train loss:0.8006121741864034\n",
            "train loss:0.7725118848110981\n",
            "train loss:0.5645082145061804\n",
            "train loss:0.6084480711593492\n",
            "train loss:0.7042740010158574\n",
            "train loss:0.6617859129735977\n",
            "train loss:0.44356848710127494\n",
            "train loss:0.5319774709381622\n",
            "train loss:0.536848951206952\n",
            "train loss:0.6308412964275703\n",
            "train loss:0.5921771656181476\n",
            "train loss:0.4340198537689131\n",
            "train loss:0.4869934306173289\n",
            "=== epoch:2, train acc:0.827, test acc:0.786 ===\n",
            "train loss:0.5387687495230405\n",
            "train loss:0.48267032911964464\n",
            "train loss:0.4722462638081979\n",
            "train loss:0.4805889416267085\n",
            "train loss:0.43064434651835076\n",
            "train loss:0.4518435552458529\n",
            "train loss:0.5894148915055486\n",
            "train loss:0.4431079986766755\n",
            "train loss:0.5329513317596277\n",
            "train loss:0.4504985405585357\n",
            "train loss:0.3973602628705121\n",
            "train loss:0.45001892528880516\n",
            "train loss:0.4418127587331411\n",
            "train loss:0.48056099003093294\n",
            "train loss:0.36319077453728155\n",
            "train loss:0.4265578423494027\n",
            "train loss:0.34134709230932936\n",
            "train loss:0.5886130481577893\n",
            "train loss:0.20459354028003168\n",
            "train loss:0.5251346883885484\n",
            "train loss:0.5011591225920369\n",
            "train loss:0.3905013245635409\n",
            "train loss:0.28961836892225823\n",
            "train loss:0.37555537942226225\n",
            "train loss:0.2826280684404546\n",
            "train loss:0.24431443733269334\n",
            "train loss:0.3278528736927688\n",
            "train loss:0.43269697897363285\n",
            "train loss:0.24804533613029822\n",
            "train loss:0.3640528455530604\n",
            "train loss:0.2579650463781318\n",
            "train loss:0.3298550027076784\n",
            "train loss:0.44229422738651875\n",
            "train loss:0.2690249918310707\n",
            "train loss:0.3113480933163358\n",
            "train loss:0.3682762764828307\n",
            "train loss:0.24296322195592573\n",
            "train loss:0.24818412232322953\n",
            "train loss:0.41006285245748425\n",
            "train loss:0.19004947418016943\n",
            "train loss:0.245053010996447\n",
            "train loss:0.38002295498993094\n",
            "train loss:0.35677757238430824\n",
            "train loss:0.2955031030130233\n",
            "train loss:0.23244696755403105\n",
            "train loss:0.22931204835100474\n",
            "train loss:0.222835552093673\n",
            "train loss:0.2543714276386943\n",
            "train loss:0.38751722077668577\n",
            "train loss:0.3147640957228712\n",
            "=== epoch:3, train acc:0.907, test acc:0.881 ===\n",
            "train loss:0.2675872372295368\n",
            "train loss:0.22040073287542217\n",
            "train loss:0.31485299016023016\n",
            "train loss:0.19752451614395253\n",
            "train loss:0.275381046171357\n",
            "train loss:0.17737012216986045\n",
            "train loss:0.3303838323384401\n",
            "train loss:0.16938712680002532\n",
            "train loss:0.231437879064909\n",
            "train loss:0.13893029677968047\n",
            "train loss:0.2411884943688759\n",
            "train loss:0.37446480753889644\n",
            "train loss:0.21667862904207585\n",
            "train loss:0.1366110327925089\n",
            "train loss:0.13017389575400184\n",
            "train loss:0.19464094850332858\n",
            "train loss:0.19231754820220515\n",
            "train loss:0.14807213932512245\n",
            "train loss:0.3419621063508517\n",
            "train loss:0.18556652343277708\n",
            "train loss:0.12088561042260718\n",
            "train loss:0.14916121069324353\n",
            "train loss:0.19300715128051904\n",
            "train loss:0.31676682287414154\n",
            "train loss:0.23792154893858272\n",
            "train loss:0.2058858110586688\n",
            "train loss:0.19164082909694094\n",
            "train loss:0.1483749935396452\n",
            "train loss:0.19248358779874908\n",
            "train loss:0.26570709782022583\n",
            "train loss:0.18093901185027395\n",
            "train loss:0.2109305911998154\n",
            "train loss:0.16726752486105173\n",
            "train loss:0.31227799023040714\n",
            "train loss:0.18236427151150686\n",
            "train loss:0.2567473851861646\n",
            "train loss:0.08616104341474723\n",
            "train loss:0.2467173981902516\n",
            "train loss:0.181783194068444\n",
            "train loss:0.17515773545722857\n",
            "train loss:0.10165727577278587\n",
            "train loss:0.1612267863748547\n",
            "train loss:0.23051418125091136\n",
            "train loss:0.18296142159012463\n",
            "train loss:0.07157225371171166\n",
            "train loss:0.14046328175786083\n",
            "train loss:0.2347393297866248\n",
            "train loss:0.1097172653123702\n",
            "train loss:0.15979797494667639\n",
            "train loss:0.20416715179427655\n",
            "=== epoch:4, train acc:0.933, test acc:0.922 ===\n",
            "train loss:0.3133773676886105\n",
            "train loss:0.26788063414902613\n",
            "train loss:0.11423254735506153\n",
            "train loss:0.2720778841986333\n",
            "train loss:0.24299546057960625\n",
            "train loss:0.15787644698040998\n",
            "train loss:0.214721828016188\n",
            "train loss:0.10371697938255764\n",
            "train loss:0.11408353410115371\n",
            "train loss:0.1479528568879357\n",
            "train loss:0.193395845531317\n",
            "train loss:0.10879436674417922\n",
            "train loss:0.1365585799416721\n",
            "train loss:0.1066052366925387\n",
            "train loss:0.18722696290719743\n",
            "train loss:0.13600545596015834\n",
            "train loss:0.12063721259425936\n",
            "train loss:0.16373503561259728\n",
            "train loss:0.1374358279355716\n",
            "train loss:0.21082832465370566\n",
            "train loss:0.17128687922040423\n",
            "train loss:0.07612790496674653\n",
            "train loss:0.17222936997107027\n",
            "train loss:0.13428542043445732\n",
            "train loss:0.17057720324568926\n",
            "train loss:0.18795962950353357\n",
            "train loss:0.14433302700960404\n",
            "train loss:0.16791441053725278\n",
            "train loss:0.21506886014232535\n",
            "train loss:0.12303219893326015\n",
            "train loss:0.17698559399040165\n",
            "train loss:0.12797765757872873\n",
            "train loss:0.21291340855890514\n",
            "train loss:0.1289834360952553\n",
            "train loss:0.10196239042326376\n",
            "train loss:0.06424751419055791\n",
            "train loss:0.2123777569258357\n",
            "train loss:0.07478317461217106\n",
            "train loss:0.213209640788774\n",
            "train loss:0.14306550862642176\n",
            "train loss:0.10346361510356918\n",
            "train loss:0.13133965852288063\n",
            "train loss:0.14478509032172782\n",
            "train loss:0.11726177885661841\n",
            "train loss:0.11118277790336889\n",
            "train loss:0.0912747677078529\n",
            "train loss:0.13018088791818996\n",
            "train loss:0.1348465232503658\n",
            "train loss:0.08066570867236257\n",
            "train loss:0.14992024337864457\n",
            "=== epoch:5, train acc:0.947, test acc:0.924 ===\n",
            "train loss:0.13353006184838467\n",
            "train loss:0.15372454266853147\n",
            "train loss:0.105696207199162\n",
            "train loss:0.07902656420864007\n",
            "train loss:0.18361241956973562\n",
            "train loss:0.12945719574129957\n",
            "train loss:0.2650386908650248\n",
            "train loss:0.19953334853213348\n",
            "train loss:0.1120920733587712\n",
            "train loss:0.10221085068792829\n",
            "train loss:0.16759739662609074\n",
            "train loss:0.08731626910225311\n",
            "train loss:0.09920003460059418\n",
            "train loss:0.12284787791574134\n",
            "train loss:0.19586107834516395\n",
            "train loss:0.13515003073282578\n",
            "train loss:0.15812588625984628\n",
            "train loss:0.11468739289460984\n",
            "train loss:0.14797217025676215\n",
            "train loss:0.09308348873940117\n",
            "train loss:0.12211095903499027\n",
            "train loss:0.05628581347252897\n",
            "train loss:0.10010799795574425\n",
            "train loss:0.09608375039101112\n",
            "train loss:0.0853770410680973\n",
            "train loss:0.16997556557272087\n",
            "train loss:0.115272953307861\n",
            "train loss:0.10427865069499545\n",
            "train loss:0.06658349644628875\n",
            "train loss:0.07215266405330542\n",
            "train loss:0.1262226705744234\n",
            "train loss:0.048607216445278996\n",
            "train loss:0.1310046369647387\n",
            "train loss:0.11192275491358611\n",
            "train loss:0.1380169419828192\n",
            "train loss:0.10779907497163034\n",
            "train loss:0.13071863007095844\n",
            "train loss:0.052979559825687694\n",
            "train loss:0.0812576694658877\n",
            "train loss:0.05996584328725805\n",
            "train loss:0.1024708523109454\n",
            "train loss:0.07254614112767319\n",
            "train loss:0.07867221873166402\n",
            "train loss:0.08244415772515992\n",
            "train loss:0.1748039307705927\n",
            "train loss:0.10431578584482855\n",
            "train loss:0.14599279784977964\n",
            "train loss:0.06717214901234012\n",
            "train loss:0.12251829311711622\n",
            "train loss:0.059344323991606635\n",
            "=== epoch:6, train acc:0.963, test acc:0.937 ===\n",
            "train loss:0.12548836787416126\n",
            "train loss:0.2313711574198441\n",
            "train loss:0.09223698545580815\n",
            "train loss:0.09234026835001675\n",
            "train loss:0.09980379906166069\n",
            "train loss:0.06449675316947064\n",
            "train loss:0.1509906067497956\n",
            "train loss:0.06649747336490523\n",
            "train loss:0.15314360206473307\n",
            "train loss:0.06720951749512734\n",
            "train loss:0.07353110860436457\n",
            "train loss:0.22267307979379358\n",
            "train loss:0.10934030985089734\n",
            "train loss:0.13308712784229967\n",
            "train loss:0.11565119886301103\n",
            "train loss:0.09039326667983401\n",
            "train loss:0.0749325057661631\n",
            "train loss:0.06953758004552103\n",
            "train loss:0.05250712139556136\n",
            "train loss:0.14971610326985862\n",
            "train loss:0.08962572361196142\n",
            "train loss:0.04618983752561796\n",
            "train loss:0.06160938186403149\n",
            "train loss:0.08841400696868919\n",
            "train loss:0.04529134294694977\n",
            "train loss:0.03330814557257137\n",
            "train loss:0.07122727928471767\n",
            "train loss:0.08344280693564286\n",
            "train loss:0.1443257369699123\n",
            "train loss:0.03217613234185745\n",
            "train loss:0.0624036371644167\n",
            "train loss:0.1139182844898915\n",
            "train loss:0.04073783296576055\n",
            "train loss:0.1386243949890461\n",
            "train loss:0.0729069190338525\n",
            "train loss:0.05485186361730187\n",
            "train loss:0.11852918440953704\n",
            "train loss:0.09656955824323\n",
            "train loss:0.11248340471072699\n",
            "train loss:0.10523219291510343\n",
            "train loss:0.03304371369041637\n",
            "train loss:0.10998535751336551\n",
            "train loss:0.06341494233065316\n",
            "train loss:0.09987626488363648\n",
            "train loss:0.08920257084775927\n",
            "train loss:0.05056877165463274\n",
            "train loss:0.15622479662590996\n",
            "train loss:0.1137654189114784\n",
            "train loss:0.0651384619739693\n",
            "train loss:0.080551009707874\n",
            "=== epoch:7, train acc:0.968, test acc:0.946 ===\n",
            "train loss:0.035414139287508195\n",
            "train loss:0.09307033854644323\n",
            "train loss:0.13552082318053682\n",
            "train loss:0.04411677939903024\n",
            "train loss:0.04045019081502327\n",
            "train loss:0.08345202199489958\n",
            "train loss:0.12929098494219002\n",
            "train loss:0.04766773640902275\n",
            "train loss:0.03622234920656106\n",
            "train loss:0.17764439308739807\n",
            "train loss:0.1005159349553387\n",
            "train loss:0.042862541663995585\n",
            "train loss:0.07328221094554506\n",
            "train loss:0.03914445181305105\n",
            "train loss:0.081645885581135\n",
            "train loss:0.08216291775704881\n",
            "train loss:0.05984911212462455\n",
            "train loss:0.05196375449668253\n",
            "train loss:0.0838122888883032\n",
            "train loss:0.12768776548073546\n",
            "train loss:0.05820154135069283\n",
            "train loss:0.052437958413220596\n",
            "train loss:0.08303208461642232\n",
            "train loss:0.14847492272233265\n",
            "train loss:0.12377467734895906\n",
            "train loss:0.07233121199371331\n",
            "train loss:0.08076965236706664\n",
            "train loss:0.057253644864680056\n",
            "train loss:0.12000507589925903\n",
            "train loss:0.04397248918397316\n",
            "train loss:0.15861329487737932\n",
            "train loss:0.037645133000711616\n",
            "train loss:0.06052208926762817\n",
            "train loss:0.08414851379770262\n",
            "train loss:0.09072536307576035\n",
            "train loss:0.04130816316115573\n",
            "train loss:0.08050183591323062\n",
            "train loss:0.03623350655904487\n",
            "train loss:0.07791132510441502\n",
            "train loss:0.08662015094751636\n",
            "train loss:0.04528561912079556\n",
            "train loss:0.10300036076814152\n",
            "train loss:0.06312717884376258\n",
            "train loss:0.04485935209204582\n",
            "train loss:0.1364418660001409\n",
            "train loss:0.2024127425726588\n",
            "train loss:0.08909372342743593\n",
            "train loss:0.17215792327227017\n",
            "train loss:0.04682045334493526\n",
            "train loss:0.03740122771288535\n",
            "=== epoch:8, train acc:0.976, test acc:0.95 ===\n",
            "train loss:0.0414684985644494\n",
            "train loss:0.10741916074142491\n",
            "train loss:0.11061322543258895\n",
            "train loss:0.10791842646848447\n",
            "train loss:0.032124952365980036\n",
            "train loss:0.057008715969330194\n",
            "train loss:0.06752415390498918\n",
            "train loss:0.04872080239376067\n",
            "train loss:0.039610450866768204\n",
            "train loss:0.06139417692080319\n",
            "train loss:0.03975938345744084\n",
            "train loss:0.16451666586130717\n",
            "train loss:0.038006733870738\n",
            "train loss:0.05012573989076687\n",
            "train loss:0.07689904183297404\n",
            "train loss:0.052563237627332754\n",
            "train loss:0.04866426473786242\n",
            "train loss:0.11039739120851223\n",
            "train loss:0.05754460703364612\n",
            "train loss:0.08298708491670428\n",
            "train loss:0.0681428369855906\n",
            "train loss:0.061398253141134965\n",
            "train loss:0.05590668788335606\n",
            "train loss:0.0878268854336105\n",
            "train loss:0.039134562158063685\n",
            "train loss:0.091818224859653\n",
            "train loss:0.09005556955051568\n",
            "train loss:0.074541687396935\n",
            "train loss:0.08904642729186847\n",
            "train loss:0.1322027080563667\n",
            "train loss:0.05708114817180683\n",
            "train loss:0.09339076845289128\n",
            "train loss:0.047847981071689256\n",
            "train loss:0.0408113797440293\n",
            "train loss:0.08388257705679869\n",
            "train loss:0.057200777114195765\n",
            "train loss:0.07351083355344469\n",
            "train loss:0.035533024186578645\n",
            "train loss:0.08746180418121659\n",
            "train loss:0.04052321774135446\n",
            "train loss:0.023735060726741447\n",
            "train loss:0.05832471133997334\n",
            "train loss:0.07811046615975721\n",
            "train loss:0.019245001557438357\n",
            "train loss:0.047620113510275175\n",
            "train loss:0.09841166367986504\n",
            "train loss:0.01984690231828996\n",
            "train loss:0.06188292416593025\n",
            "train loss:0.07840576509225522\n",
            "train loss:0.054402544874444334\n",
            "=== epoch:9, train acc:0.975, test acc:0.957 ===\n",
            "train loss:0.033767806517596304\n",
            "train loss:0.027104631068992132\n",
            "train loss:0.06740902828452311\n",
            "train loss:0.13601450151157088\n",
            "train loss:0.04943396557785751\n",
            "train loss:0.035046774840809626\n",
            "train loss:0.04352691630869255\n",
            "train loss:0.044302636537011185\n",
            "train loss:0.029707358588667097\n",
            "train loss:0.0545524739459672\n",
            "train loss:0.010671715924417471\n",
            "train loss:0.03938577317889214\n",
            "train loss:0.09553376125352676\n",
            "train loss:0.023107348028512505\n",
            "train loss:0.03227436874577436\n",
            "train loss:0.04474422627890871\n",
            "train loss:0.0504259971370756\n",
            "train loss:0.06730106792870724\n",
            "train loss:0.06901711470293055\n",
            "train loss:0.017603952737491362\n",
            "train loss:0.04339622214431161\n",
            "train loss:0.020943042009907903\n",
            "train loss:0.11803310305989231\n",
            "train loss:0.05428174612704303\n",
            "train loss:0.042187145954056426\n",
            "train loss:0.04146299123233318\n",
            "train loss:0.045179377362072\n",
            "train loss:0.02398023354060106\n",
            "train loss:0.0382208052154218\n",
            "train loss:0.08211935273839364\n",
            "train loss:0.07200560623550965\n",
            "train loss:0.007072921088515999\n",
            "train loss:0.055362934952606094\n",
            "train loss:0.047779071759957284\n",
            "train loss:0.030905929608520396\n",
            "train loss:0.044573292703281114\n",
            "train loss:0.08116226129373098\n",
            "train loss:0.04258762596368777\n",
            "train loss:0.02407266837827544\n",
            "train loss:0.028973460324786853\n",
            "train loss:0.06066026011269598\n",
            "train loss:0.03580877986724737\n",
            "train loss:0.1087286057713901\n",
            "train loss:0.08244869255179506\n",
            "train loss:0.05335224145350284\n",
            "train loss:0.03646955257681797\n",
            "train loss:0.04619787617797438\n",
            "train loss:0.0595947397474206\n",
            "train loss:0.04301747621539181\n",
            "train loss:0.03728054494480735\n",
            "=== epoch:10, train acc:0.99, test acc:0.956 ===\n",
            "train loss:0.03927629468885324\n",
            "train loss:0.0591230039840873\n",
            "train loss:0.03784202817214667\n",
            "train loss:0.05447445239022569\n",
            "train loss:0.028676805832153738\n",
            "train loss:0.03103682065738814\n",
            "train loss:0.06456085443479537\n",
            "train loss:0.02673081446391877\n",
            "train loss:0.0394935870535078\n",
            "train loss:0.028508878680698607\n",
            "train loss:0.026649284547680008\n",
            "train loss:0.06339231036486977\n",
            "train loss:0.09336547165790274\n",
            "train loss:0.01743768112737612\n",
            "train loss:0.03278208075892452\n",
            "train loss:0.06464750027105415\n",
            "train loss:0.032665658070795626\n",
            "train loss:0.026699687819010602\n",
            "train loss:0.0846868430794874\n",
            "train loss:0.02447711441533851\n",
            "train loss:0.05893154461295249\n",
            "train loss:0.059551770421453265\n",
            "train loss:0.019803661946165618\n",
            "train loss:0.026461388108740044\n",
            "train loss:0.07632546887290925\n",
            "train loss:0.04726625994851557\n",
            "train loss:0.01403577939555575\n",
            "train loss:0.040466951362855365\n",
            "train loss:0.026145498022727493\n",
            "train loss:0.0325232280841963\n",
            "train loss:0.018983464000466185\n",
            "train loss:0.054788459050647526\n",
            "train loss:0.036149084531403905\n",
            "train loss:0.07438740378073205\n",
            "train loss:0.06820877764804685\n",
            "train loss:0.04470277233039722\n",
            "train loss:0.09256305742972555\n",
            "train loss:0.07849873378459436\n",
            "train loss:0.03915878317950048\n",
            "train loss:0.08282693995926137\n",
            "train loss:0.02603844304338256\n",
            "train loss:0.027080222887871735\n",
            "train loss:0.06235388900466601\n",
            "train loss:0.0206882825487029\n",
            "train loss:0.030844252509675645\n",
            "train loss:0.03112640886006885\n",
            "train loss:0.016412339462335994\n",
            "train loss:0.03880452148288694\n",
            "train loss:0.032707964972717364\n",
            "train loss:0.01602438158380758\n",
            "=== epoch:11, train acc:0.987, test acc:0.958 ===\n",
            "train loss:0.0344903057018258\n",
            "train loss:0.028845301223365878\n",
            "train loss:0.03479682981974819\n",
            "train loss:0.02284935308971317\n",
            "train loss:0.029690650570802224\n",
            "train loss:0.028386707562086916\n",
            "train loss:0.012548059213243347\n",
            "train loss:0.08635056943075531\n",
            "train loss:0.0497194570889051\n",
            "train loss:0.030800038971040503\n",
            "train loss:0.02958899763996391\n",
            "train loss:0.051071030587446116\n",
            "train loss:0.007330518938886724\n",
            "train loss:0.03188676327573159\n",
            "train loss:0.07789867798611866\n",
            "train loss:0.046638773725492196\n",
            "train loss:0.03345079463908488\n",
            "train loss:0.030933083303271748\n",
            "train loss:0.02039270020098998\n",
            "train loss:0.09190900414800618\n",
            "train loss:0.041089840726169086\n",
            "train loss:0.029365394460478166\n",
            "train loss:0.07359544560578869\n",
            "train loss:0.020928822846529204\n",
            "train loss:0.019303741464069366\n",
            "train loss:0.041491379907049014\n",
            "train loss:0.0598938869452946\n",
            "train loss:0.040270343059945916\n",
            "train loss:0.06758652623136853\n",
            "train loss:0.025546499363050482\n",
            "train loss:0.020915248693846533\n",
            "train loss:0.01804207799445279\n",
            "train loss:0.05066414318048517\n",
            "train loss:0.0179461540379584\n",
            "train loss:0.02059935527593867\n",
            "train loss:0.046458864238761485\n",
            "train loss:0.01946809184331239\n",
            "train loss:0.039561608327920494\n",
            "train loss:0.017200631411803557\n",
            "train loss:0.05356869354076433\n",
            "train loss:0.018757541409570158\n",
            "train loss:0.03703839152765933\n",
            "train loss:0.030248191721332238\n",
            "train loss:0.01195255335663403\n",
            "train loss:0.015483826582888202\n",
            "train loss:0.012177517749594241\n",
            "train loss:0.013447200318340506\n",
            "train loss:0.025503471492403843\n",
            "train loss:0.06170037932511458\n",
            "train loss:0.011096558507435746\n",
            "=== epoch:12, train acc:0.991, test acc:0.959 ===\n",
            "train loss:0.026755379148455126\n",
            "train loss:0.018456588241901897\n",
            "train loss:0.03774212372107679\n",
            "train loss:0.024705391335388377\n",
            "train loss:0.03372960702665417\n",
            "train loss:0.021627015976473832\n",
            "train loss:0.02240367001930059\n",
            "train loss:0.03942856972625304\n",
            "train loss:0.01525278141314966\n",
            "train loss:0.08703693491062901\n",
            "train loss:0.01302306067234556\n",
            "train loss:0.033632059826584036\n",
            "train loss:0.03381665015141451\n",
            "train loss:0.020610471567721878\n",
            "train loss:0.02672980269682255\n",
            "train loss:0.03236524962132685\n",
            "train loss:0.029792305482387385\n",
            "train loss:0.017417181292540423\n",
            "train loss:0.03471718117045809\n",
            "train loss:0.029898849132869255\n",
            "train loss:0.04697579130727699\n",
            "train loss:0.026156648819259422\n",
            "train loss:0.021512886202200345\n",
            "train loss:0.02320206124801051\n",
            "train loss:0.02554546051276134\n",
            "train loss:0.020545657702825196\n",
            "train loss:0.009298620957499984\n",
            "train loss:0.06102400458884926\n",
            "train loss:0.05275374301011486\n",
            "train loss:0.03503593341841573\n",
            "train loss:0.033801084576296506\n",
            "train loss:0.01930153952984404\n",
            "train loss:0.02298957675915777\n",
            "train loss:0.08012662139381024\n",
            "train loss:0.08602450365658912\n",
            "train loss:0.03460809854695476\n",
            "train loss:0.03498996780167127\n",
            "train loss:0.03028697970248716\n",
            "train loss:0.03241948247428659\n",
            "train loss:0.02958040641578662\n",
            "train loss:0.03203967818084164\n",
            "train loss:0.04201830532521587\n",
            "train loss:0.0374705249800808\n",
            "train loss:0.029218867538574583\n",
            "train loss:0.01840190181886407\n",
            "train loss:0.033808979985978864\n",
            "train loss:0.02023325763269169\n",
            "train loss:0.08902227056114867\n",
            "train loss:0.041196110253767466\n",
            "train loss:0.04656803508099817\n",
            "=== epoch:13, train acc:0.99, test acc:0.959 ===\n",
            "train loss:0.10110697332550124\n",
            "train loss:0.031618727554933404\n",
            "train loss:0.04472229073134652\n",
            "train loss:0.009875928550075973\n",
            "train loss:0.011584370204328556\n",
            "train loss:0.03359089013230168\n",
            "train loss:0.015745483211305995\n",
            "train loss:0.023597022199064965\n",
            "train loss:0.011132375980345166\n",
            "train loss:0.019075423885769452\n",
            "train loss:0.020411449071951793\n",
            "train loss:0.031435018656875105\n",
            "train loss:0.01562268690487562\n",
            "train loss:0.03222083286143242\n",
            "train loss:0.01745267120743721\n",
            "train loss:0.0645233116517758\n",
            "train loss:0.02064457515406848\n",
            "train loss:0.015068496370019032\n",
            "train loss:0.0398796409548614\n",
            "train loss:0.015008841669409287\n",
            "train loss:0.02958884482454224\n",
            "train loss:0.043902279987176024\n",
            "train loss:0.02577825731190468\n",
            "train loss:0.007455217337729522\n",
            "train loss:0.051815551383017924\n",
            "train loss:0.05206906519453433\n",
            "train loss:0.013607648556828743\n",
            "train loss:0.016935905703773797\n",
            "train loss:0.013732972986372856\n",
            "train loss:0.03269788910037306\n",
            "train loss:0.011191711488407523\n",
            "train loss:0.007124893024767088\n",
            "train loss:0.015870940428367487\n",
            "train loss:0.0163988601652492\n",
            "train loss:0.0268398962278542\n",
            "train loss:0.019892091679414185\n",
            "train loss:0.010009435565874665\n",
            "train loss:0.008136038572583479\n",
            "train loss:0.009446402369494077\n",
            "train loss:0.010542873271130866\n",
            "train loss:0.012455850453857246\n",
            "train loss:0.013003624071694607\n",
            "train loss:0.011884100507271426\n",
            "train loss:0.006573174241050453\n",
            "train loss:0.007740553790578293\n",
            "train loss:0.01707113356142389\n",
            "train loss:0.006824866466038781\n",
            "train loss:0.018208118057573372\n",
            "train loss:0.019882776652435846\n",
            "train loss:0.010043562768104287\n",
            "=== epoch:14, train acc:0.995, test acc:0.96 ===\n",
            "train loss:0.013131958584786916\n",
            "train loss:0.02434480178085266\n",
            "train loss:0.007524267645653209\n",
            "train loss:0.029246007333230947\n",
            "train loss:0.050826869394402605\n",
            "train loss:0.015466578556044155\n",
            "train loss:0.02610316352557182\n",
            "train loss:0.012357923572864691\n",
            "train loss:0.01840305783235151\n",
            "train loss:0.010610916842433323\n",
            "train loss:0.009455018175501132\n",
            "train loss:0.010662782429065907\n",
            "train loss:0.01003505510178921\n",
            "train loss:0.026425118692159875\n",
            "train loss:0.013088607323134698\n",
            "train loss:0.018991660334367153\n",
            "train loss:0.009078987451716756\n",
            "train loss:0.017701013952652198\n",
            "train loss:0.017022752179962467\n",
            "train loss:0.013826312349014937\n",
            "train loss:0.02642226042778855\n",
            "train loss:0.016304868534915716\n",
            "train loss:0.012555025727950359\n",
            "train loss:0.018985454768236973\n",
            "train loss:0.009268453873711965\n",
            "train loss:0.009830918108688807\n",
            "train loss:0.02749560314905806\n",
            "train loss:0.00933788853687753\n",
            "train loss:0.012079773999999339\n",
            "train loss:0.008612189159520221\n",
            "train loss:0.029537442937557685\n",
            "train loss:0.013037887569261497\n",
            "train loss:0.021833083314774113\n",
            "train loss:0.02168182673561172\n",
            "train loss:0.017770173873737985\n",
            "train loss:0.04108105852300449\n",
            "train loss:0.014641871477222394\n",
            "train loss:0.01717839264564935\n",
            "train loss:0.026069884792855495\n",
            "train loss:0.010392123086964569\n",
            "train loss:0.011741405036732364\n",
            "train loss:0.016474410438054442\n",
            "train loss:0.012422217846513796\n",
            "train loss:0.04910721897492929\n",
            "train loss:0.015422384814618249\n",
            "train loss:0.006161816623144646\n",
            "train loss:0.019838109764328465\n",
            "train loss:0.014844017118309261\n",
            "train loss:0.03059993350913175\n",
            "train loss:0.004968278527052554\n",
            "=== epoch:15, train acc:0.998, test acc:0.963 ===\n",
            "train loss:0.010428833551461125\n",
            "train loss:0.021962144913377286\n",
            "train loss:0.021595662878354228\n",
            "train loss:0.0329702326046593\n",
            "train loss:0.010248014270014504\n",
            "train loss:0.01611180894746327\n",
            "train loss:0.009064819178763537\n",
            "train loss:0.02329815960262246\n",
            "train loss:0.03196739192692057\n",
            "train loss:0.01807614773340612\n",
            "train loss:0.02779831629891974\n",
            "train loss:0.007512477006134004\n",
            "train loss:0.030228340757388263\n",
            "train loss:0.014260671523043897\n",
            "train loss:0.016618699304724156\n",
            "train loss:0.00909476019623851\n",
            "train loss:0.005960902596114649\n",
            "train loss:0.005365119609470154\n",
            "train loss:0.004954439987224919\n",
            "train loss:0.0039741612035768484\n",
            "train loss:0.033150801410458314\n",
            "train loss:0.014696021030093473\n",
            "train loss:0.029294055956639586\n",
            "train loss:0.014519545400672353\n",
            "train loss:0.008601585017728288\n",
            "train loss:0.01689059024979471\n",
            "train loss:0.013282156505377176\n",
            "train loss:0.00894002515469018\n",
            "train loss:0.010815440013822444\n",
            "train loss:0.01852772053993721\n",
            "train loss:0.011075624569418296\n",
            "train loss:0.005660752145860409\n",
            "train loss:0.009661644723935995\n",
            "train loss:0.006143398827727989\n",
            "train loss:0.015689753686311626\n",
            "train loss:0.008805293695203439\n",
            "train loss:0.011669765938358521\n",
            "train loss:0.014321591963105989\n",
            "train loss:0.0068784178341525426\n",
            "train loss:0.013150998092772292\n",
            "train loss:0.015828549219245047\n",
            "train loss:0.01205102568855583\n",
            "train loss:0.02300198959165475\n",
            "train loss:0.010335318344756936\n",
            "train loss:0.0024652286417492005\n",
            "train loss:0.010139999808879361\n",
            "train loss:0.00970854288566384\n",
            "train loss:0.006403489701807899\n",
            "train loss:0.008541795644535216\n",
            "train loss:0.009022964252534437\n",
            "=== epoch:16, train acc:0.998, test acc:0.959 ===\n",
            "train loss:0.018237461542337508\n",
            "train loss:0.01130287439392684\n",
            "train loss:0.024935633867764758\n",
            "train loss:0.005521444732272201\n",
            "train loss:0.009898630463479325\n",
            "train loss:0.003678854980919592\n",
            "train loss:0.02105302940513643\n",
            "train loss:0.008475377760374832\n",
            "train loss:0.01665519417545402\n",
            "train loss:0.009797647886448314\n",
            "train loss:0.010402023646174631\n",
            "train loss:0.008488612275866274\n",
            "train loss:0.005712664655987079\n",
            "train loss:0.021245622570910637\n",
            "train loss:0.01887857987820748\n",
            "train loss:0.004870484790855955\n",
            "train loss:0.008027948318814043\n",
            "train loss:0.008050439513958141\n",
            "train loss:0.01282085034740273\n",
            "train loss:0.00509876410971481\n",
            "train loss:0.008932967435891705\n",
            "train loss:0.009281021144717575\n",
            "train loss:0.005732435069428979\n",
            "train loss:0.01239658347415517\n",
            "train loss:0.012940154140332812\n",
            "train loss:0.009266795053946729\n",
            "train loss:0.005462737711591098\n",
            "train loss:0.003984783897108348\n",
            "train loss:0.007709117012648789\n",
            "train loss:0.003585577649833162\n",
            "train loss:0.008724535868185796\n",
            "train loss:0.01953709117327815\n",
            "train loss:0.010612589526401534\n",
            "train loss:0.012479115053377787\n",
            "train loss:0.005415111884481727\n",
            "train loss:0.01643888153025121\n",
            "train loss:0.014711209375460626\n",
            "train loss:0.010424016099495684\n",
            "train loss:0.026113037584922387\n",
            "train loss:0.015643289881403427\n",
            "train loss:0.00505775829811398\n",
            "train loss:0.0025381596776079627\n",
            "train loss:0.01058644338459997\n",
            "train loss:0.0104441275695734\n",
            "train loss:0.009694739081027382\n",
            "train loss:0.01070169317141511\n",
            "train loss:0.03984171576927555\n",
            "train loss:0.01208799366252512\n",
            "train loss:0.004769132333998055\n",
            "train loss:0.012766200045405643\n",
            "=== epoch:17, train acc:1.0, test acc:0.959 ===\n",
            "train loss:0.005487057652791288\n",
            "train loss:0.01004964418741572\n",
            "train loss:0.005197606595597943\n",
            "train loss:0.007563612564933797\n",
            "train loss:0.03225221622377657\n",
            "train loss:0.01872952425306109\n",
            "train loss:0.012150805861585224\n",
            "train loss:0.00610611958896705\n",
            "train loss:0.01506108420939352\n",
            "train loss:0.008460973292660571\n",
            "train loss:0.01574958626307316\n",
            "train loss:0.010187235733517628\n",
            "train loss:0.009915687979784064\n",
            "train loss:0.010503328262377442\n",
            "train loss:0.009636310234937736\n",
            "train loss:0.02492995912546579\n",
            "train loss:0.012918362442300933\n",
            "train loss:0.00952458117616308\n",
            "train loss:0.01586910503311227\n",
            "train loss:0.006547600127736519\n",
            "train loss:0.009477052989244412\n",
            "train loss:0.009075305855621843\n",
            "train loss:0.09473521659638828\n",
            "train loss:0.014014478146054948\n",
            "train loss:0.004192936697858415\n",
            "train loss:0.009920816471872675\n",
            "train loss:0.02084004608040242\n",
            "train loss:0.007464430231566626\n",
            "train loss:0.020983424528637454\n",
            "train loss:0.008977978237160417\n",
            "train loss:0.01501001312571599\n",
            "train loss:0.017097072537854825\n",
            "train loss:0.004453662996701841\n",
            "train loss:0.009368998498315555\n",
            "train loss:0.0076928001914540476\n",
            "train loss:0.007611371047045098\n",
            "train loss:0.01725468136782996\n",
            "train loss:0.013814000919119962\n",
            "train loss:0.007900025994526784\n",
            "train loss:0.05219981271911343\n",
            "train loss:0.01916812730185645\n",
            "train loss:0.03178216944864829\n",
            "train loss:0.018078482929469338\n",
            "train loss:0.009225491527228086\n",
            "train loss:0.006429673783157555\n",
            "train loss:0.03078374748761922\n",
            "train loss:0.02804823136256298\n",
            "train loss:0.020352659299575367\n",
            "train loss:0.008260923974800313\n",
            "train loss:0.014752586631184436\n",
            "=== epoch:18, train acc:0.999, test acc:0.958 ===\n",
            "train loss:0.009064583351367233\n",
            "train loss:0.008603460556247272\n",
            "train loss:0.012974245255729003\n",
            "train loss:0.0124326427691973\n",
            "train loss:0.004500158513674477\n",
            "train loss:0.015579974248883723\n",
            "train loss:0.006306750155472048\n",
            "train loss:0.013125475984194484\n",
            "train loss:0.005957355889804105\n",
            "train loss:0.01004419857936916\n",
            "train loss:0.008169735468067903\n",
            "train loss:0.012473114979678257\n",
            "train loss:0.002923975433768659\n",
            "train loss:0.006525578644934095\n",
            "train loss:0.011856501868997832\n",
            "train loss:0.010297624913102188\n",
            "train loss:0.002628683004078808\n",
            "train loss:0.0019646026263063386\n",
            "train loss:0.010548888184262134\n",
            "train loss:0.0038513402799952097\n",
            "train loss:0.0062551474411880114\n",
            "train loss:0.00894026959888234\n",
            "train loss:0.016514700407281643\n",
            "train loss:0.006465779703967939\n",
            "train loss:0.0064963527077556\n",
            "train loss:0.004785757238220546\n",
            "train loss:0.012509001032173658\n",
            "train loss:0.0067541544212574075\n",
            "train loss:0.008487198462438535\n",
            "train loss:0.013970668530031243\n",
            "train loss:0.010935674863339084\n",
            "train loss:0.01608656272602331\n",
            "train loss:0.0153389115870203\n",
            "train loss:0.0074976749538796595\n",
            "train loss:0.007074791274113661\n",
            "train loss:0.015529288887699267\n",
            "train loss:0.006350446938685194\n",
            "train loss:0.0401858878845183\n",
            "train loss:0.013300542523820067\n",
            "train loss:0.008151860693834922\n",
            "train loss:0.006487242005470368\n",
            "train loss:0.01100275622129407\n",
            "train loss:0.008439766204861657\n",
            "train loss:0.0027950441739395208\n",
            "train loss:0.014768087465153654\n",
            "train loss:0.011382175059290347\n",
            "train loss:0.004591827494786411\n",
            "train loss:0.006820468942100262\n",
            "train loss:0.006177841430908968\n",
            "train loss:0.008958961538516615\n",
            "=== epoch:19, train acc:0.997, test acc:0.959 ===\n",
            "train loss:0.0062564580438782295\n",
            "train loss:0.017905424056300093\n",
            "train loss:0.0024824768919566673\n",
            "train loss:0.006746896573193336\n",
            "train loss:0.005303009762625329\n",
            "train loss:0.007012942731719498\n",
            "train loss:0.013541771671064\n",
            "train loss:0.005415937596988184\n",
            "train loss:0.006250114302468678\n",
            "train loss:0.016327995778734554\n",
            "train loss:0.005901826261508788\n",
            "train loss:0.014965371129028189\n",
            "train loss:0.007941151488413012\n",
            "train loss:0.003916938110002675\n",
            "train loss:0.00857205979773223\n",
            "train loss:0.01193362294329881\n",
            "train loss:0.007825636992476308\n",
            "train loss:0.008943864422921192\n",
            "train loss:0.023338303756293696\n",
            "train loss:0.011022423761094862\n",
            "train loss:0.008392729618251145\n",
            "train loss:0.008037442459060346\n",
            "train loss:0.006744610784890462\n",
            "train loss:0.012742288350584816\n",
            "train loss:0.004983330164020587\n",
            "train loss:0.005043493765884739\n",
            "train loss:0.006432896483478667\n",
            "train loss:0.015516321168599545\n",
            "train loss:0.00806138833105969\n",
            "train loss:0.012907112890173447\n",
            "train loss:0.004073671980702249\n",
            "train loss:0.002838378077629413\n",
            "train loss:0.009806063443875121\n",
            "train loss:0.01066861045767726\n",
            "train loss:0.006609816953914822\n",
            "train loss:0.0135932646864794\n",
            "train loss:0.0042837350229489035\n",
            "train loss:0.0044570830075083305\n",
            "train loss:0.010609815775095607\n",
            "train loss:0.008344126112037787\n",
            "train loss:0.00396028215846346\n",
            "train loss:0.0060861216372044905\n",
            "train loss:0.009545148940457455\n",
            "train loss:0.01012898285360981\n",
            "train loss:0.002402819052278397\n",
            "train loss:0.009059733584477111\n",
            "train loss:0.004539406054980839\n",
            "train loss:0.006225256852142972\n",
            "train loss:0.0030344414837151228\n",
            "train loss:0.008983590040605345\n",
            "=== epoch:20, train acc:1.0, test acc:0.961 ===\n",
            "train loss:0.007648434782170183\n",
            "train loss:0.014244001233036895\n",
            "train loss:0.007238723534549012\n",
            "train loss:0.005702278906811793\n",
            "train loss:0.006359533587813301\n",
            "train loss:0.00818427621565173\n",
            "train loss:0.00566502821157592\n",
            "train loss:0.005351862138377176\n",
            "train loss:0.005834196885208453\n",
            "train loss:0.006510698411301688\n",
            "train loss:0.0035425512348099724\n",
            "train loss:0.0034447186811033463\n",
            "train loss:0.0016728065625077162\n",
            "train loss:0.00732816312459692\n",
            "train loss:0.014858808199067564\n",
            "train loss:0.00827019299897219\n",
            "train loss:0.004259393718360369\n",
            "train loss:0.005351129100119695\n",
            "train loss:0.003906843797439183\n",
            "train loss:0.003093199661026353\n",
            "train loss:0.007235551149601373\n",
            "train loss:0.003463607531465978\n",
            "train loss:0.013579972689487487\n",
            "train loss:0.00669671593194951\n",
            "train loss:0.008038611515414892\n",
            "train loss:0.006288321941842978\n",
            "train loss:0.004641570972442996\n",
            "train loss:0.005733273812382927\n",
            "train loss:0.00330629999026838\n",
            "train loss:0.0034097851601367197\n",
            "train loss:0.005754250986936973\n",
            "train loss:0.012915817320068584\n",
            "train loss:0.004098934908787965\n",
            "train loss:0.008608749450024279\n",
            "train loss:0.002718057392594128\n",
            "train loss:0.0039048849512398143\n",
            "train loss:0.00579102948723537\n",
            "train loss:0.004679956100297207\n",
            "train loss:0.03785923871372974\n",
            "train loss:0.0035896623381790806\n",
            "train loss:0.0034465789974592793\n",
            "train loss:0.003932857125037065\n",
            "train loss:0.00400275706650395\n",
            "train loss:0.015226582166171542\n",
            "train loss:0.00621826675657371\n",
            "train loss:0.004925717294295004\n",
            "train loss:0.0032172768049490968\n",
            "train loss:0.007939480451836416\n",
            "train loss:0.009419729145375396\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.964\n",
            "Saved Network Parameters!\n",
            "\n",
            "filter_num:500\n",
            "filter_size:10\n",
            "pad: 1\n",
            "stride: 4\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from ch06.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "# 7-3, 10-4-패딩1, 13-5, 21-7\n",
        "filter_num = 500\n",
        "filter_size = 10\n",
        "pad = 1\n",
        "stride = 4\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':filter_num, 'filter_size':filter_size, \\\n",
        "                            'pad':pad, 'stride':stride},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                    epochs=max_epochs, mini_batch_size=100,\n",
        "                    optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                    evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 학습 전 무작위(랜덤)의 가중치 생성된 filter 표시\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "print(f\"\"\"\n",
        "filter_num:{filter_num}\n",
        "filter_size:{filter_size}\n",
        "pad: {pad}\n",
        "stride: {stride}\n",
        "\"\"\")\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
